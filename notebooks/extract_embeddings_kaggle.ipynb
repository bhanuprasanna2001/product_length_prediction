{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd58fe62",
   "metadata": {},
   "source": [
    "# Extract Embeddings for e5small and allmpnet\n",
    "\n",
    "This notebook is designed to run on Kaggle to extract embeddings using GPU acceleration.\n",
    "\n",
    "**Models to extract:**\n",
    "- `e5small`: intfloat/multilingual-e5-small (384d)\n",
    "- `allmpnet`: sentence-transformers/all-mpnet-base-v2 (768d)\n",
    "\n",
    "**Instructions:**\n",
    "1. Upload your `total_sentence_train.csv` and `total_sentence_test.csv` to Kaggle as a dataset\n",
    "2. Enable GPU accelerator (Settings → Accelerator → GPU)\n",
    "3. Run all cells\n",
    "4. Download the generated `.npy` files from the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce7b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982377b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb881f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - Update these paths based on your Kaggle dataset\n",
    "# ============================================================================\n",
    "\n",
    "# Update this path to match your Kaggle dataset location\n",
    "# Example: \"/kaggle/input/your-dataset-name/total_sentence_train.csv\"\n",
    "TRAIN_DATA_PATH = \"/kaggle/input/amazon-ml-challenge-data/total_sentence_train.csv\"\n",
    "TEST_DATA_PATH = \"/kaggle/input/amazon-ml-challenge-data/total_sentence_test.csv\"\n",
    "\n",
    "# Output directory (Kaggle working directory)\n",
    "OUTPUT_DIR = Path(\"/kaggle/working\")\n",
    "\n",
    "# Models to extract (the two remaining ones)\n",
    "MODELS_TO_EXTRACT = {\n",
    "    \"e5small\": \"intfloat/multilingual-e5-small\",        # 384d\n",
    "    \"allmpnet\": \"sentence-transformers/all-mpnet-base-v2\",  # 768d\n",
    "}\n",
    "\n",
    "# Batch size (increase if you have more GPU memory)\n",
    "BATCH_SIZE = 256  # Kaggle T4 can handle larger batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117967e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(texts, model_key, model_path, batch_size=256):\n",
    "    \"\"\"\n",
    "    Extract embeddings for a list of texts.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "        model_key: Short name for the model\n",
    "        model_path: HuggingFace model path\n",
    "        batch_size: Batch size for encoding\n",
    "        \n",
    "    Returns:\n",
    "        Embeddings array (n_texts, dim) in float16\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Loading {model_key}: {model_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = SentenceTransformer(model_path, device=device)\n",
    "    \n",
    "    # E5 models require \"query: \" prefix for best performance\n",
    "    if \"e5\" in model_key.lower():\n",
    "        print(\"Adding 'query: ' prefix for E5 model...\")\n",
    "        texts = [f\"query: {t}\" for t in texts]\n",
    "    \n",
    "    print(f\"Encoding {len(texts):,} texts on {device}...\")\n",
    "    embeddings = model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "    \n",
    "    # Convert to float16 to save storage (halves the file size)\n",
    "    embeddings = embeddings.astype(np.float16)\n",
    "    print(f\"Shape: {embeddings.shape}, dtype: {embeddings.dtype}\")\n",
    "    \n",
    "    # Cleanup to free GPU memory\n",
    "    del model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def save_embeddings(embeddings, model_key, split, output_dir):\n",
    "    \"\"\"Save embeddings to disk as .npy file.\"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    filepath = output_dir / f\"{model_key}_{split}.npy\"\n",
    "    np.save(filepath, embeddings)\n",
    "    \n",
    "    size_mb = filepath.stat().st_size / (1024 * 1024)\n",
    "    print(f\"✓ Saved: {filepath} ({size_mb:.1f} MB)\")\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0639e8f",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "print(\"Loading training data...\")\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH)\n",
    "print(f\"Train samples: {len(train_df):,}\")\n",
    "print(f\"Columns: {train_df.columns.tolist()}\")\n",
    "\n",
    "# Extract texts\n",
    "train_texts = train_df[\"TOTAL_SENTENCE\"].fillna(\"\").tolist()\n",
    "print(f\"\\nSample text: {train_texts[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29941c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "print(\"Loading test data...\")\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "print(f\"Test samples: {len(test_df):,}\")\n",
    "\n",
    "# Extract texts\n",
    "test_texts = test_df[\"TOTAL_SENTENCE\"].fillna(\"\").tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82e5434",
   "metadata": {},
   "source": [
    "## Extract Embeddings - e5small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47290e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract e5small embeddings for TRAIN\n",
    "e5small_train = extract_embeddings(\n",
    "    train_texts, \n",
    "    \"e5small\", \n",
    "    MODELS_TO_EXTRACT[\"e5small\"],\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "save_embeddings(e5small_train, \"e5small\", \"train\", OUTPUT_DIR)\n",
    "\n",
    "# Free memory\n",
    "del e5small_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3aa586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract e5small embeddings for TEST\n",
    "e5small_test = extract_embeddings(\n",
    "    test_texts, \n",
    "    \"e5small\", \n",
    "    MODELS_TO_EXTRACT[\"e5small\"],\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "save_embeddings(e5small_test, \"e5small\", \"test\", OUTPUT_DIR)\n",
    "\n",
    "# Free memory\n",
    "del e5small_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1386dace",
   "metadata": {},
   "source": [
    "## Extract Embeddings - allmpnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a90657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract allmpnet embeddings for TRAIN\n",
    "allmpnet_train = extract_embeddings(\n",
    "    train_texts, \n",
    "    \"allmpnet\", \n",
    "    MODELS_TO_EXTRACT[\"allmpnet\"],\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "save_embeddings(allmpnet_train, \"allmpnet\", \"train\", OUTPUT_DIR)\n",
    "\n",
    "# Free memory\n",
    "del allmpnet_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f89680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract allmpnet embeddings for TEST\n",
    "allmpnet_test = extract_embeddings(\n",
    "    test_texts, \n",
    "    \"allmpnet\", \n",
    "    MODELS_TO_EXTRACT[\"allmpnet\"],\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "save_embeddings(allmpnet_test, \"allmpnet\", \"test\", OUTPUT_DIR)\n",
    "\n",
    "# Free memory\n",
    "del allmpnet_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8d5063",
   "metadata": {},
   "source": [
    "## Verify Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52787554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all generated files\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATED FILES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for f in OUTPUT_DIR.glob(\"*.npy\"):\n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    arr = np.load(f)\n",
    "    print(f\"{f.name}: shape={arr.shape}, dtype={arr.dtype}, size={size_mb:.1f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DONE! Download the .npy files from the Output tab.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ef40fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Quick sanity check - verify embeddings are normalized\n",
    "print(\"\\nSanity Check - Embedding Norms (should be ~1.0):\")\n",
    "for f in OUTPUT_DIR.glob(\"*.npy\"):\n",
    "    arr = np.load(f)\n",
    "    norms = np.linalg.norm(arr.astype(np.float32), axis=1)\n",
    "    print(f\"{f.name}: mean_norm={norms.mean():.4f}, std={norms.std():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
