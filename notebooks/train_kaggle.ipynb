{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5484054,"sourceType":"datasetVersion","datasetId":3159862}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"d04a6fd3","cell_type":"markdown","source":"# Product Length Prediction\nText Encoder + Product Type Embedding → MLP → Product Length","metadata":{}},{"id":"c0d6f3af","cell_type":"code","source":"# Install dependencies (Colab)\n!pip install -q pytorch-lightning transformers wandb sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T08:38:15.080204Z","iopub.execute_input":"2026-01-22T08:38:15.080482Z","iopub.status.idle":"2026-01-22T08:38:18.687402Z","shell.execute_reply.started":"2026-01-22T08:38:15.080450Z","shell.execute_reply":"2026-01-22T08:38:18.686596Z"}},"outputs":[],"execution_count":1},{"id":"44278aaa","cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nimport pytorch_lightning as pl\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\nfrom tqdm import tqdm\nimport wandb\n\nwandb.login()\n\npl.seed_everything(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T08:38:48.139401Z","iopub.execute_input":"2026-01-22T08:38:48.140045Z","iopub.status.idle":"2026-01-22T08:38:48.375634Z","shell.execute_reply.started":"2026-01-22T08:38:48.140011Z","shell.execute_reply":"2026-01-22T08:38:48.374987Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhanu-prasanna2001\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\nSeed set to 42\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"42"},"metadata":{}}],"execution_count":4},{"id":"736d50e7","cell_type":"code","source":"# Update these paths to your Drive location\nDATA_DIR = '/kaggle/input/amazon-ml-challenge-2023/total_sentence_data/total_sentence_data/'\nTRAIN_PATH = DATA_DIR + 'total_sentence_train.csv'\nTEST_PATH = DATA_DIR + 'total_sentence_test.csv'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T08:38:51.066144Z","iopub.execute_input":"2026-01-22T08:38:51.066446Z","iopub.status.idle":"2026-01-22T08:38:51.070622Z","shell.execute_reply.started":"2026-01-22T08:38:51.066417Z","shell.execute_reply":"2026-01-22T08:38:51.069757Z"}},"outputs":[],"execution_count":5},{"id":"75696878","cell_type":"markdown","source":"## Config","metadata":{}},{"id":"4e9edbf0","cell_type":"code","source":"from dataclasses import dataclass\n\n@dataclass\nclass Config:\n    train_path: str = TRAIN_PATH\n    test_path: str = TEST_PATH\n    max_length: int = 256\n    text_encoder: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n    product_type_emb_dim: int = 64\n    hidden_dims: tuple = (256, 64)\n    dropout: float = 0.1\n    batch_size: int = 64\n    lr: float = 2e-5\n    weight_decay: float = 0.01\n    epochs: int = 2\n    warmup_ratio: float = 0.1\n    num_workers: int = 2\n    seed: int = 42\n\nconfig = Config()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T08:38:55.032238Z","iopub.execute_input":"2026-01-22T08:38:55.032603Z","iopub.status.idle":"2026-01-22T08:38:55.038582Z","shell.execute_reply.started":"2026-01-22T08:38:55.032571Z","shell.execute_reply":"2026-01-22T08:38:55.037778Z"}},"outputs":[],"execution_count":6},{"id":"7543fe37","cell_type":"markdown","source":"## Dataset","metadata":{}},{"id":"5589eb60","cell_type":"code","source":"class ProductDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length, product_type_map, is_test=False):\n        self.texts = df['TOTAL_SENTENCE'].tolist()\n        self.product_types = df['PRODUCT_TYPE_ID'].map(product_type_map).fillna(0).astype(int).tolist()\n        self.targets = None if is_test else df['PRODUCT_LENGTH'].tolist()\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.is_test = is_test\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = self.tokenizer(\n            self.texts[idx],\n            truncation=True,\n            max_length=self.max_length,\n            padding='max_length',\n            return_tensors='pt'\n        )\n        item = {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'product_type': torch.tensor(self.product_types[idx], dtype=torch.long)\n        }\n        if not self.is_test:\n            item['target'] = torch.tensor(self.targets[idx], dtype=torch.float32)\n        return item","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T08:38:56.432201Z","iopub.execute_input":"2026-01-22T08:38:56.432547Z","iopub.status.idle":"2026-01-22T08:38:56.439360Z","shell.execute_reply.started":"2026-01-22T08:38:56.432477Z","shell.execute_reply":"2026-01-22T08:38:56.438570Z"}},"outputs":[],"execution_count":7},{"id":"ff14fc57","cell_type":"code","source":"class ProductDataModule(pl.LightningDataModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.tokenizer = AutoTokenizer.from_pretrained(config.text_encoder)\n        self.product_type_map = None\n\n    def setup(self, stage=None):\n        train_df = pd.read_csv(self.config.train_path)\n        \n        # Shuffle the dataframe for random split\n        train_df = train_df.sample(frac=1, random_state=self.config.seed).reset_index(drop=True)\n        \n        all_types = train_df['PRODUCT_TYPE_ID'].unique()\n        self.product_type_map = {t: i+1 for i, t in enumerate(all_types)}\n        self.num_product_types = len(all_types) + 1\n        \n        # Split train/val/test (80/10/10)\n        n = len(train_df)\n        train_end = int(0.8 * n)\n        val_end = int(0.9 * n)\n        \n        self.train_df = train_df.iloc[:train_end]\n        self.val_df = train_df.iloc[train_end:val_end]\n        self.test_df = train_df.iloc[val_end:]\n        \n        self.train_ds = ProductDataset(\n            self.train_df, self.tokenizer, self.config.max_length, self.product_type_map\n        )\n        self.val_ds = ProductDataset(\n            self.val_df, self.tokenizer, self.config.max_length, self.product_type_map\n        )\n        self.test_ds = ProductDataset(\n            self.test_df, self.tokenizer, self.config.max_length, self.product_type_map\n        )\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_ds,\n            batch_size=self.config.batch_size,\n            shuffle=True,\n            num_workers=self.config.num_workers,\n            pin_memory=True,\n            persistent_workers=True\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.val_ds,\n            batch_size=self.config.batch_size,\n            shuffle=False,\n            num_workers=self.config.num_workers,\n            pin_memory=True,\n            persistent_workers=True\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            self.test_ds,\n            batch_size=self.config.batch_size,\n            shuffle=False,\n            num_workers=self.config.num_workers,\n            pin_memory=True,\n            persistent_workers=True\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T08:38:57.812975Z","iopub.execute_input":"2026-01-22T08:38:57.813291Z","iopub.status.idle":"2026-01-22T08:38:57.823314Z","shell.execute_reply.started":"2026-01-22T08:38:57.813261Z","shell.execute_reply":"2026-01-22T08:38:57.822522Z"}},"outputs":[],"execution_count":8},{"id":"f518a1d5","cell_type":"markdown","source":"## Model","metadata":{}},{"id":"43411bc2","cell_type":"code","source":"class ProductLengthModel(pl.LightningModule):\n    def __init__(self, config, num_product_types):\n        super().__init__()\n        self.save_hyperparameters()\n        self.config = config\n\n        self.text_encoder = AutoModel.from_pretrained(config.text_encoder)\n        self.text_dim = self.text_encoder.config.hidden_size\n\n        self.product_emb = nn.Embedding(num_product_types, config.product_type_emb_dim)\n\n        input_dim = self.text_dim + config.product_type_emb_dim\n        layers = []\n        for hidden_dim in config.hidden_dims:\n            layers.extend([\n                nn.Linear(input_dim, hidden_dim),\n                nn.ReLU(),\n                nn.Dropout(config.dropout)\n            ])\n            input_dim = hidden_dim\n        layers.append(nn.Linear(input_dim, 1))\n        self.head = nn.Sequential(*layers)\n\n    def forward(self, input_ids, attention_mask, product_type):\n        text_out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n        text_emb = (text_out.last_hidden_state * attention_mask.unsqueeze(-1)).sum(1)\n        text_emb = text_emb / attention_mask.sum(-1, keepdim=True)\n\n        type_emb = self.product_emb(product_type)\n\n        combined = torch.cat([text_emb, type_emb], dim=-1)\n        return self.head(combined).squeeze(-1)\n\n    def _step(self, batch, stage):\n        pred = self(batch['input_ids'], batch['attention_mask'], batch['product_type'])\n        target = batch['target']\n\n        pred_log = torch.log1p(torch.clamp(pred, min=0))\n        target_log = torch.log1p(target)\n        loss = nn.functional.mse_loss(pred_log, target_log)\n\n        with torch.no_grad():\n            mape = torch.mean(torch.abs((target - pred) / target)) * 100\n            rmsle = torch.sqrt(loss)\n\n        self.log(f'{stage}_loss', loss, prog_bar=True)\n        self.log(f'{stage}_mape', mape, prog_bar=True)\n        self.log(f'{stage}_rmsle', rmsle, prog_bar=True)\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        return self._step(batch, 'train')\n\n    def validation_step(self, batch, batch_idx):\n        return self._step(batch, 'val')\n\n    def test_step(self, batch, batch_idx):\n        return self._step(batch, 'test')\n\n    def configure_optimizers(self):\n        encoder_params = list(self.text_encoder.parameters())\n        other_params = list(self.product_emb.parameters()) + list(self.head.parameters())\n\n        optimizer = torch.optim.AdamW([\n            {'params': encoder_params, 'lr': self.config.lr * 0.1},\n            {'params': other_params, 'lr': self.config.lr}\n        ], weight_decay=self.config.weight_decay)\n\n        total_steps = self.trainer.estimated_stepping_batches\n        warmup_steps = int(self.config.warmup_ratio * total_steps)\n\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer,\n            max_lr=[self.config.lr * 0.1, self.config.lr],\n            total_steps=total_steps,\n            pct_start=warmup_steps / total_steps\n        )\n\n        return {\n            'optimizer': optimizer,\n            'lr_scheduler': {'scheduler': scheduler, 'interval': 'step'}\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T08:38:59.732460Z","iopub.execute_input":"2026-01-22T08:38:59.732896Z","iopub.status.idle":"2026-01-22T08:38:59.747315Z","shell.execute_reply.started":"2026-01-22T08:38:59.732852Z","shell.execute_reply":"2026-01-22T08:38:59.746582Z"}},"outputs":[],"execution_count":9},{"id":"4961f3ad","cell_type":"markdown","source":"## Training","metadata":{}},{"id":"57bc7b44","cell_type":"code","source":"# Initialize data module\ndm = ProductDataModule(config)\ndm.setup()\n\nprint(f\"Train samples: {len(dm.train_ds)}\")\nprint(f\"Val samples: {len(dm.val_ds)}\")\nprint(f\"Test samples: {len(dm.test_ds)}\")\nprint(f\"Product types: {dm.num_product_types}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T08:39:03.113900Z","iopub.execute_input":"2026-01-22T08:39:03.114194Z","iopub.status.idle":"2026-01-22T08:39:24.291444Z","shell.execute_reply.started":"2026-01-22T08:39:03.114166Z","shell.execute_reply":"2026-01-22T08:39:24.290536Z"}},"outputs":[{"name":"stdout","text":"Train samples: 1738559\nVal samples: 217320\nTest samples: 217320\nProduct types: 12773\n","output_type":"stream"}],"execution_count":10},{"id":"2b45f476","cell_type":"code","source":"# Initialize model\nmodel = ProductLengthModel(config, dm.num_product_types)\nprint(f\"Text encoder dim: {model.text_dim}\")\nprint(f\"Combined dim: {model.text_dim + config.product_type_emb_dim}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T08:39:24.292908Z","iopub.execute_input":"2026-01-22T08:39:24.293202Z","iopub.status.idle":"2026-01-22T08:39:28.051263Z","shell.execute_reply.started":"2026-01-22T08:39:24.293170Z","shell.execute_reply":"2026-01-22T08:39:28.050547Z"}},"outputs":[{"name":"stderr","text":"2026-01-22 08:39:25.047714: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769071165.087741     253 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769071165.100618     253 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769071165.130299     253 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769071165.130323     253 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769071165.130329     253 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769071165.130334     253 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Text encoder dim: 384\nCombined dim: 448\n","output_type":"stream"}],"execution_count":11},{"id":"de649944","cell_type":"code","source":"# Setup logger and callbacks\nwandb_logger = WandbLogger(\n    project=\"amazon-product-length\",\n    name=\"text_encoder_v1\",\n    config=config.__dict__\n)\n\ncallbacks = [\n    ModelCheckpoint(\n        dirpath=\"checkpoints\",\n        filename=\"best-{epoch}-{val_rmsle:.4f}\",\n        monitor=\"val_rmsle\",\n        mode=\"min\",\n        save_top_k=1\n    ),\n    EarlyStopping(monitor=\"val_rmsle\", patience=3, mode=\"min\"),\n    LearningRateMonitor(logging_interval=\"step\")\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T08:39:43.381978Z","iopub.execute_input":"2026-01-22T08:39:43.382706Z","iopub.status.idle":"2026-01-22T08:39:43.391114Z","shell.execute_reply.started":"2026-01-22T08:39:43.382669Z","shell.execute_reply":"2026-01-22T08:39:43.390413Z"}},"outputs":[],"execution_count":12},{"id":"c3295d14","cell_type":"code","source":"# Initialize trainer\ntrainer = pl.Trainer(\n    max_epochs=config.epochs,\n    accelerator=\"auto\",\n    devices=1,\n    logger=wandb_logger,\n    callbacks=callbacks,\n    gradient_clip_val=1.0,\n    accumulate_grad_batches=2,\n    val_check_interval=0.5,\n    log_every_n_steps=50\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T08:40:13.823960Z","iopub.execute_input":"2026-01-22T08:40:13.824641Z","iopub.status.idle":"2026-01-22T08:40:13.881376Z","shell.execute_reply.started":"2026-01-22T08:40:13.824607Z","shell.execute_reply":"2026-01-22T08:40:13.880711Z"}},"outputs":[{"name":"stderr","text":"GPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\n","output_type":"stream"}],"execution_count":13},{"id":"59eeaf1e","cell_type":"code","source":"# Train!\ntrainer.fit(model, dm)\n\n# Test on held-out test set from train data\nprint(\"\\n--- Testing on held-out test set ---\")\ntrainer.test(model, dm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T08:40:17.929179Z","iopub.execute_input":"2026-01-22T08:40:17.929726Z","iopub.status.idle":"2026-01-22T12:45:08.381131Z","shell.execute_reply.started":"2026-01-22T08:40:17.929690Z","shell.execute_reply":"2026-01-22T12:45:08.380573Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.22.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>./wandb/run-20260122_084018-v6hrfqwk</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bhanu-prasanna2001/amazon-product-length/runs/v6hrfqwk' target=\"_blank\">text_encoder_v1</a></strong> to <a href='https://wandb.ai/bhanu-prasanna2001/amazon-product-length' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bhanu-prasanna2001/amazon-product-length' target=\"_blank\">https://wandb.ai/bhanu-prasanna2001/amazon-product-length</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bhanu-prasanna2001/amazon-product-length/runs/v6hrfqwk' target=\"_blank\">https://wandb.ai/bhanu-prasanna2001/amazon-product-length/runs/v6hrfqwk</a>"},"metadata":{}},{"name":"stderr","text":"LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nLoading `train_dataloader` to estimate number of stepping batches.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"┏━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName        \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m┃\n┡━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ text_encoder │ BertModel  │ 22.7 M │ eval  │     0 │\n│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ product_emb  │ Embedding  │  817 K │ train │     0 │\n│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ head         │ Sequential │  131 K │ train │     0 │\n└───┴──────────────┴────────────┴────────┴───────┴───────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name         </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>┃\n┡━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ text_encoder │ BertModel  │ 22.7 M │ eval  │     0 │\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ product_emb  │ Embedding  │  817 K │ train │     0 │\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ head         │ Sequential │  131 K │ train │     0 │\n└───┴──────────────┴────────────┴────────┴───────┴───────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mTrainable params\u001b[0m: 23.7 M                                                                                           \n\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n\u001b[1mTotal params\u001b[0m: 23.7 M                                                                                               \n\u001b[1mTotal estimated model params size (MB)\u001b[0m: 94                                                                         \n\u001b[1mModules in train mode\u001b[0m: 9                                                                                           \n\u001b[1mModules in eval mode\u001b[0m: 120                                                                                          \n\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 23.7 M                                                                                           \n<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n<span style=\"font-weight: bold\">Total params</span>: 23.7 M                                                                                               \n<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 94                                                                         \n<span style=\"font-weight: bold\">Modules in train mode</span>: 9                                                                                           \n<span style=\"font-weight: bold\">Modules in eval mode</span>: 120                                                                                          \n<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bef6c01557e4639943352ff3df0b15c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/usr/local/lib/python3.12/dist-packages/pytorch_lightning/loops/fit_loop.py:534: Found 120 module(s) in eval mode \nat the start of training. This may lead to unexpected behavior during training. If this is intentional, you can \nignore this warning.\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.12/dist-packages/pytorch_lightning/loops/fit_loop.py:534: Found 120 module(s) in eval mode \nat the start of training. This may lead to unexpected behavior during training. If this is intentional, you can \nignore this warning.\n</pre>\n"},"metadata":{}},{"name":"stderr","text":"`Trainer.fit` stopped: `max_epochs=2` reached.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"name":"stdout","text":"\n--- Testing on held-out test set ---\n","output_type":"stream"},{"name":"stderr","text":"LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c65604385b34059ba56884094d6dacc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5071058869361877    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m        test_mape        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   156.53073120117188    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m       test_rmsle        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6964890956878662    \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5071058869361877     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_mape         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    156.53073120117188     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">        test_rmsle         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6964890956878662     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[{'test_loss': 0.5071058869361877,\n  'test_mape': 156.53073120117188,\n  'test_rmsle': 0.6964890956878662}]"},"metadata":{}}],"execution_count":14},{"id":"600bea26","cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"aa2d750d","cell_type":"markdown","source":"## Inference","metadata":{}},{"id":"46bdd7ab","cell_type":"code","source":"def predict(model, config, dm):\n    model.eval()\n    model.cuda()\n    \n    test_df = pd.read_csv(config.test_path)\n    tokenizer = AutoTokenizer.from_pretrained(config.text_encoder)\n    test_ds = ProductDataset(test_df, tokenizer, config.max_length, dm.product_type_map, is_test=True)\n    test_loader = DataLoader(test_ds, batch_size=config.batch_size * 2, shuffle=False, num_workers=2)\n    \n    predictions = []\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Predicting\"):\n            pred = model(\n                batch['input_ids'].cuda(),\n                batch['attention_mask'].cuda(),\n                batch['product_type'].cuda()\n            )\n            predictions.extend(pred.cpu().numpy().tolist())\n    \n    submission = pd.DataFrame({\n        'PRODUCT_ID': test_df['PRODUCT_ID'],\n        'PRODUCT_LENGTH': predictions\n    })\n    return submission","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5430aed7","cell_type":"code","source":"# Snapping function - round predictions to nearest training value\ndef snap_to_nearest(pred, values):\n    idx = np.searchsorted(values, pred)\n    if idx == 0: return values[0]\n    if idx == len(values): return values[-1]\n    before, after = values[idx-1], values[idx]\n    return before if (pred - before) < (after - pred) else after","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7dab5d4b","cell_type":"code","source":"# Load best checkpoint and predict\nbest_model = ProductLengthModel.load_from_checkpoint(\n    trainer.checkpoint_callback.best_model_path,\n    config=config,\n    num_product_types=dm.num_product_types,\n    weights_only=False\n)\n\n# Get raw predictions\nsubmission = predict(best_model, config, dm)\n\n# Load train data to get unique lengths for snapping\ntrain_df = pd.read_csv(config.train_path)\ntrain_lengths = sorted(train_df['PRODUCT_LENGTH'].unique())\n\n# Apply snapping\nsubmission['PRODUCT_LENGTH'] = submission['PRODUCT_LENGTH'].apply(\n    lambda x: snap_to_nearest(x, train_lengths)\n)\n\nsubmission.to_csv('submission.csv', index=False)\nprint(f\"Saved submission.csv with {len(submission)} predictions\")\nsubmission.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b902313f","cell_type":"code","source":"# Copy submission to Drive\n!cp submission.csv /content/drive/MyDrive/amazon-ml-challenge/","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}