{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04a6fd3",
   "metadata": {},
   "source": [
    "# Product Length Prediction\n",
    "Text Encoder + Product Type Embedding → MLP → Product Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d6f3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab)\n",
    "!pip install -q pytorch-lightning transformers wandb sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44278aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736d50e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for Colab)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Update these paths to your Drive location\n",
    "DATA_DIR = '/content/drive/MyDrive/amazon-ml-challenge/data/total_sentence_data/total_sentence_data/'\n",
    "TRAIN_PATH = DATA_DIR + 'total_sentence_train.csv'\n",
    "TEST_PATH = DATA_DIR + 'total_sentence_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75696878",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9edbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    train_path: str = TRAIN_PATH\n",
    "    test_path: str = TEST_PATH\n",
    "    max_length: int = 256\n",
    "    text_encoder: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    product_type_emb_dim: int = 64\n",
    "    hidden_dims: tuple = (256, 64)\n",
    "    dropout: float = 0.1\n",
    "    batch_size: int = 64\n",
    "    lr: float = 2e-5\n",
    "    weight_decay: float = 0.01\n",
    "    epochs: int = 5\n",
    "    warmup_ratio: float = 0.1\n",
    "    num_workers: int = 2\n",
    "    seed: int = 42\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7543fe37",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5589eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length, product_type_map, is_test=False):\n",
    "        self.texts = df['TOTAL_SENTENCE'].tolist()\n",
    "        self.product_types = df['PRODUCT_TYPE_ID'].map(product_type_map).fillna(0).astype(int).tolist()\n",
    "        self.targets = None if is_test else df['PRODUCT_LENGTH'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'product_type': torch.tensor(self.product_types[idx], dtype=torch.long)\n",
    "        }\n",
    "        if not self.is_test:\n",
    "            item['target'] = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff14fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.text_encoder)\n",
    "        self.product_type_map = None\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        train_df = pd.read_csv(self.config.train_path)\n",
    "        \n",
    "        all_types = train_df['PRODUCT_TYPE_ID'].unique()\n",
    "        self.product_type_map = {t: i+1 for i, t in enumerate(all_types)}\n",
    "        self.num_product_types = len(all_types) + 1\n",
    "        \n",
    "        val_size = int(0.1 * len(train_df))\n",
    "        self.train_df = train_df.iloc[:-val_size]\n",
    "        self.val_df = train_df.iloc[-val_size:]\n",
    "        \n",
    "        self.train_ds = ProductDataset(\n",
    "            self.train_df, self.tokenizer, self.config.max_length, self.product_type_map\n",
    "        )\n",
    "        self.val_ds = ProductDataset(\n",
    "            self.val_df, self.tokenizer, self.config.max_length, self.product_type_map\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.config.num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.config.num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f518a1d5",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43411bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductLengthModel(pl.LightningModule):\n",
    "    def __init__(self, config, num_product_types):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.config = config\n",
    "\n",
    "        self.text_encoder = AutoModel.from_pretrained(config.text_encoder)\n",
    "        self.text_dim = self.text_encoder.config.hidden_size\n",
    "\n",
    "        self.product_emb = nn.Embedding(num_product_types, config.product_type_emb_dim)\n",
    "\n",
    "        input_dim = self.text_dim + config.product_type_emb_dim\n",
    "        layers = []\n",
    "        for hidden_dim in config.hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(config.dropout)\n",
    "            ])\n",
    "            input_dim = hidden_dim\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        self.head = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, product_type):\n",
    "        text_out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_emb = (text_out.last_hidden_state * attention_mask.unsqueeze(-1)).sum(1)\n",
    "        text_emb = text_emb / attention_mask.sum(-1, keepdim=True)\n",
    "\n",
    "        type_emb = self.product_emb(product_type)\n",
    "\n",
    "        combined = torch.cat([text_emb, type_emb], dim=-1)\n",
    "        return self.head(combined).squeeze(-1)\n",
    "\n",
    "    def _step(self, batch, stage):\n",
    "        pred = self(batch['input_ids'], batch['attention_mask'], batch['product_type'])\n",
    "        target = batch['target']\n",
    "\n",
    "        pred_log = torch.log1p(torch.clamp(pred, min=0))\n",
    "        target_log = torch.log1p(target)\n",
    "        loss = nn.functional.mse_loss(pred_log, target_log)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mape = torch.mean(torch.abs((target - pred) / target)) * 100\n",
    "            rmsle = torch.sqrt(loss)\n",
    "\n",
    "        self.log(f'{stage}_loss', loss, prog_bar=True)\n",
    "        self.log(f'{stage}_mape', mape, prog_bar=True)\n",
    "        self.log(f'{stage}_rmsle', rmsle, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._step(batch, 'train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._step(batch, 'val')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        encoder_params = list(self.text_encoder.parameters())\n",
    "        other_params = list(self.product_emb.parameters()) + list(self.head.parameters())\n",
    "\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': encoder_params, 'lr': self.config.lr * 0.1},\n",
    "            {'params': other_params, 'lr': self.config.lr}\n",
    "        ], weight_decay=self.config.weight_decay)\n",
    "\n",
    "        total_steps = self.trainer.estimated_stepping_batches\n",
    "        warmup_steps = int(self.config.warmup_ratio * total_steps)\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=[self.config.lr * 0.1, self.config.lr],\n",
    "            total_steps=total_steps,\n",
    "            pct_start=warmup_steps / total_steps\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {'scheduler': scheduler, 'interval': 'step'}\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4961f3ad",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3412cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to W&B\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc7b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data module\n",
    "dm = ProductDataModule(config)\n",
    "dm.setup()\n",
    "\n",
    "print(f\"Train samples: {len(dm.train_ds)}\")\n",
    "print(f\"Val samples: {len(dm.val_ds)}\")\n",
    "print(f\"Product types: {dm.num_product_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b45f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = ProductLengthModel(config, dm.num_product_types)\n",
    "print(f\"Text encoder dim: {model.text_dim}\")\n",
    "print(f\"Combined dim: {model.text_dim + config.product_type_emb_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de649944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logger and callbacks\n",
    "wandb_logger = WandbLogger(\n",
    "    project=\"amazon-product-length\",\n",
    "    name=\"text_encoder_v1\",\n",
    "    config=config.__dict__\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=\"checkpoints\",\n",
    "        filename=\"best-{epoch}-{val_rmsle:.4f}\",\n",
    "        monitor=\"val_rmsle\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=1\n",
    "    ),\n",
    "    EarlyStopping(monitor=\"val_rmsle\", patience=3, mode=\"min\"),\n",
    "    LearningRateMonitor(logging_interval=\"step\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3295d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=config.epochs,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    precision=\"16-mixed\",\n",
    "    logger=wandb_logger,\n",
    "    callbacks=callbacks,\n",
    "    gradient_clip_val=1.0,\n",
    "    accumulate_grad_batches=2,\n",
    "    val_check_interval=0.5,\n",
    "    log_every_n_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eeaf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600bea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d750d",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bdd7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, config, dm):\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    \n",
    "    test_df = pd.read_csv(config.test_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.text_encoder)\n",
    "    test_ds = ProductDataset(test_df, tokenizer, config.max_length, dm.product_type_map, is_test=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=config.batch_size * 2, shuffle=False, num_workers=2)\n",
    "    \n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "            pred = model(\n",
    "                batch['input_ids'].cuda(),\n",
    "                batch['attention_mask'].cuda(),\n",
    "                batch['product_type'].cuda()\n",
    "            )\n",
    "            predictions.extend(pred.cpu().numpy().tolist())\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'PRODUCT_ID': test_df['PRODUCT_ID'],\n",
    "        'PRODUCT_LENGTH': predictions\n",
    "    })\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5430aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snapping function - round predictions to nearest training value\n",
    "def snap_to_nearest(pred, values):\n",
    "    idx = np.searchsorted(values, pred)\n",
    "    if idx == 0: return values[0]\n",
    "    if idx == len(values): return values[-1]\n",
    "    before, after = values[idx-1], values[idx]\n",
    "    return before if (pred - before) < (after - pred) else after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dab5d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint and predict\n",
    "best_model = ProductLengthModel.load_from_checkpoint(\n",
    "    trainer.checkpoint_callback.best_model_path,\n",
    "    config=config,\n",
    "    num_product_types=dm.num_product_types,\n",
    "    weights_only=False\n",
    ")\n",
    "\n",
    "# Get raw predictions\n",
    "submission = predict(best_model, config, dm)\n",
    "\n",
    "# Load train data to get unique lengths for snapping\n",
    "train_df = pd.read_csv(config.train_path)\n",
    "train_lengths = sorted(train_df['PRODUCT_LENGTH'].unique())\n",
    "\n",
    "# Apply snapping\n",
    "submission['PRODUCT_LENGTH'] = submission['PRODUCT_LENGTH'].apply(\n",
    "    lambda x: snap_to_nearest(x, train_lengths)\n",
    ")\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"Saved submission.csv with {len(submission)} predictions\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b902313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy submission to Drive\n",
    "!cp submission.csv /content/drive/MyDrive/amazon-ml-challenge/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
